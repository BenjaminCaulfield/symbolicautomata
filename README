Welcome to the SRA Artifact VM.
A copy of our paper can be found in /home/cav/SRA/SRA.pdf

The source code for SRA can be found in /home/cav/SRA.
In this, you'll find a Maven based Java project.
The relevant modules are "SVPAlib", which contains the source code of the SRA implementation, and "SVPABenchmark", which containts tools to run and measure the preformance of SRAs.

You will also find that a "compile" and a "run" script are available to the SRA user as aliases. These are shortcuts to compile any changes to source code, and run the benchmarks, respectively.

The most relevant bits of the source code can be found in /home/cav/SRA/SVPAlib/src/automata/sra

The experiments can be found in /home/cav/SRA/SVPABenchmark/src/benchmark/SRA/Experiments.java. Other experiments, not in the paper, are included as well, but they are not run by the benchmark script. You can run those experiments as well by adding the annotation @ToRun before the corresponding method.

When we first ran these experiments we ran them on bare-metal with specifications:
CPU: 3.5 GHz Intel Core i7 (quad-core)
RAM: 16GB
JVM Memory: 8GB

We do understand not all hardware will be able to run this VM with such specs. As such, the most complex benchmarks might cause a Stack Overflow, or an OutOfMemory error, as they were designed to push our hardware, and the algorithms themselves to their limits. The benchmark script will automatically handle these situations by skipping any remaining runs of the benchmark.

We recommend you give the JVM as much memory as possible, to be able to run as many benchmarks as possible. Depending on your hardware, some of these might take a considerable amount of time to compute, as such, we have also included simpler equivalent benchmarks where appropriate.

RUNNING INSTRUCTIONS:
1. Define how much memory to give to JVM:
$ export JAVA_OPTS="-Xmx<HOW MANY GB>g"
NOTE: Please make sure there are no whitespaces before or after the equals sign.

2. Move to source directory:
$ cd /home/cav/SRA

3. Compile the source code:
$ compile

* Run/list all benchmarks:
$ run [-e <experiments>] [-f <csv file>] [-n <runs per benchmark>] [-t <timeout>] [-l] 

-h,--help                   Show usage information
-e,--experiments            Benchmarks to run. Default: All
-f,--file                   CSV file. Default: ./Experiments.csv
-n,--numberOfRuns           Number of runs per benchmark (Default: 3)
-t,--timeout                Specify the timeout in whole seconds (Default: 300)
-l                          Lists all the benchmarks

The format for the CSV file is:

"benchmarkName",<runtime 1>,...,<runtime k>,<average runtime>

where k = <runs per experiment> (or the default value)

Examples: 

* List all benchmarks:
$ run -l

* Run all benchmarks 5 times
$ run -n 5

* Run specific benchmarks 5 times with 20s timeout:
$ run -e testSSNParserSRA testXMLParserSRA -n 5 -t 20

* Run all benchmarks and write results in "newFilePath.csv". 
$ run -f newFilePath.csv


If you find a specific benchmark is taking longer than expected, feel free to stop the script. Previous results will already have been written to the file.